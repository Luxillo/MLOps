# Lab 1: Fundamentos de MLOps - Seguimiento de Experimentos con MLflow

## ğŸ¯ Lo que AprenderÃ¡s

Domina MLflow a travÃ©s de 4 tareas prÃ¡cticas progresivas:

1. **Tarea 1**: Seguimiento de experimento base (4 TODOs)
2. **Tarea 2**: Ajuste de hiperparÃ¡metros (4 TODOs)
3. **Tarea 3**: ComparaciÃ³n de mÃºltiples modelos (3 TODOs)
4. **Tarea 4**: Registro y versionado de modelos (4 TODOs)

**Total**: 15 TODOs para completar en ~50 minutos

---

## ğŸŒ Accediendo a la UI de MLflow

**DespuÃ©s de ejecutar `mlflow ui`**, para acceder:

1. **Haz clic en el menÃº hamburguesa (â‰¡)** en la esquina superior derecha
2. **Selecciona "View Port"**
3. **Ingresa el puerto**: `5000`
4. **Haz clic en "Open Port"**
5. **La UI de MLflow se abre** en una nueva pestaÃ±a del navegador

ğŸ’¡ **MantÃ©n esta pestaÃ±a abierta** - Â¡la usarÃ¡s despuÃ©s de cada tarea!

---

## ğŸ“Š El Dataset: devops_metrics.csv

PredecirÃ¡s el **tiempo de respuesta de la API** a partir de mÃ©tricas del sistema:

**CaracterÃ­sticas** (lo que monitoreas):
- Uso de CPU (%)
- Uso de memoria (%)
- E/S de disco (MB/s)
- Latencia de red (ms)
- Usuarios concurrentes (cantidad)

**Objetivo** (lo que predices):
- Tiempo de respuesta (milisegundos)

**Uso en el Mundo Real**:
- PlanificaciÃ³n de capacidad
- Decisiones de autoescalado
- Monitoreo de SLA
- OptimizaciÃ³n de rendimiento

---

## ğŸ”§ ConfiguraciÃ³n del Entorno

Todo estÃ¡ preconfigurado:

- âœ… MLflow (Ãºltima versiÃ³n estable)
- âœ… UI de MLflow ejecutÃ¡ndose en puerto 5000
- âœ… Directorio de seguimiento: `/root/mlruns`
- âœ… Dataset: `/root/code/devops_metrics.csv`
- âœ… Paquetes de Python: scikit-learn, pandas, numpy, matplotlib

---

## ğŸš€ Iniciar el Laboratorio

### 1. Activar entorno virtual
```bash
# Windows
mlops_env\Scripts\activate

# Linux/Mac
source mlops_env/bin/activate
```

### 2. Iniciar MLflow UI
```bash
# OpciÃ³n 1: Comando estÃ¡ndar
mlflow ui

# OpciÃ³n 2: Comando completo (si hay problemas)
mlflow ui --host 0.0.0.0 --port 5000
```

### 3. Verificar acceso
- âœ… Terminal muestra: `Listening at: http://127.0.0.1:5000`
- âœ… UI accesible en puerto 5000 (ver instrucciones arriba)

ğŸ’¡ **MantÃ©n la terminal abierta** - MLflow UI debe ejecutarse durante todo el lab

---

## ğŸš€ Conceptos Clave

### Â¿QuÃ© es MLflow?

MLflow resuelve el **"problema del caos de ML"**:

**Sin MLflow**:
- âŒ Â¿QuÃ© hiperparÃ¡metros dieron 95% de precisiÃ³n?
- âŒ Â¿Podemos reproducir el modelo del mes pasado?
- âŒ Â¿QuÃ© cambiÃ³ entre v1 y v2?
- âŒ Â¿CÃ³mo revertimos un modelo malo?

**Con MLflow**:
- âœ… Cada experimento registrado automÃ¡ticamente
- âœ… Seguimiento de parÃ¡metros y mÃ©tricas
- âœ… Versionado de modelos (como Git)
- âœ… ReversiÃ³n con un clic
- âœ… ColaboraciÃ³n en equipo

### Los Tres Componentes de MLflow

1. **Tracking**: Registrar experimentos (parÃ¡metros, mÃ©tricas, cÃ³digo)
2. **Model Registry**: Control de versiones para modelos
3. **Projects**: Flujos de trabajo de ML reproducibles

Este lab cubre Tracking (Tareas 1-3) y Registry (Tarea 4).

### Por quÃ© Importa el Versionado de Modelos

Escenario de producciÃ³n:
```
v1.0 (ProducciÃ³n) â†’ 92% precisiÃ³n âœ…
v1.1 (Staging)    â†’ 94% precisiÃ³n ğŸ¯ (Â¡despliega esto!)
v1.2 (Dev)        â†’ 89% precisiÃ³n âŒ (bug encontrado, no desplegar)
```

MLflow Registry te permite:
- Rastrear todas las versiones
- Etapas de modelos (Dev â†’ Staging â†’ ProducciÃ³n)
- Revertir instantÃ¡neamente si v1.1 falla

---

## ğŸ“ Resumen de Tareas

### Tarea 1: Experimento Base (15 min)
**Objetivo**: Rastrear tu primer experimento de ML

**AprenderÃ¡s**:
- Establecer nombre del experimento
- Iniciar ejecuciÃ³n de MLflow
- Registrar parÃ¡metros (model_type, random_state)
- Registrar mÃ©tricas (RMSE, RÂ², MAE)

**DespuÃ©s de completar**: Abrir UI de MLflow â†’ Â¡Ver tu experimento!

---

### Tarea 2: Ajuste de HiperparÃ¡metros (15 min)
**Objetivo**: Comparar RandomForest con diferentes configuraciones

**AprenderÃ¡s**:
- Registrar parÃ¡metros individuales (`log_param`)
- Registrar parÃ¡metros en lote (`log_params`)
- Registrar mÃºltiples mÃ©tricas
- Etiquetar experimentos para organizaciÃ³n

**DespuÃ©s de completar**: UI de MLflow â†’ Comparar ejecuciones â†’ Â¡Encontrar mejor configuraciÃ³n!

---

### Tarea 3: ComparaciÃ³n de Modelos (10 min)
**Objetivo**: Entrenar 3 modelos, elegir el mejor

**Modelos**:
- LinearRegression (rÃ¡pido, simple)
- RandomForest (poderoso, ensemble)
- GradientBoosting (lento, preciso)

**AprenderÃ¡s**:
- Comparar arquitecturas de modelos
- Registrar mÃºltiples mÃ©tricas a la vez
- Guardar artefactos de modelos

**DespuÃ©s de completar**: UI de MLflow â†’ Ordenar por RÂ² â†’ Â¡Ver ganador!

---

### Tarea 4: Registro de Modelos (10 min)
**Objetivo**: Registrar el mejor modelo para producciÃ³n

**AprenderÃ¡s**:
- Registrar modelo en MLflow Registry
- Asignar versiones de modelo
- Transicionar a etapa "Staging"
- Cargar modelo registrado para predicciones

**DespuÃ©s de completar**: UI de MLflow â†’ PestaÃ±a Models â†’ Â¡Ver modelo registrado!

---

## ğŸ§ª Pruebas del Modelo (Bonus)

**Objetivo**: Probar tu modelo registrado como API REST

**UbicaciÃ³n**: Directorio `test/`

**MÃ©todos disponibles**:
1. **AutomÃ¡tico**: `python test/test_model_deployment.py`
2. **Manual**: `python test/serve_model.py` + `python test/test_requests.py`
3. **Curl**: `mlflow models serve` + comandos curl

**AprenderÃ¡s**:
- Servir modelos MLflow como API
- Enviar requests HTTP al modelo
- Probar diferentes escenarios de carga
- Validar predicciones en tiempo real

**DespuÃ©s de completar**: Â¡Modelo listo para producciÃ³n!

### ğŸš€ Prueba RÃ¡pida del Modelo

```bash
# Terminal 1: Servir modelo
python serve_model_local.py

# Terminal 2: Probar modelo
python test_model_api.py
```

---

## ğŸ’¡ Consejos Pro

1. **Usa `Ctrl+G` / `Cmd+G`** para saltar a nÃºmeros de lÃ­nea
2. **Lee los comentarios TODO** - contienen pistas
3. **Revisa la UI de MLflow despuÃ©s de cada tarea** - visualiza tu progreso
4. **Compara ejecuciones** - usa la tabla de comparaciÃ³n de la UI
5. **Experimenta libremente** - Â¡MLflow rastrea todo!

---

## ğŸ“ Aplicaciones del Mundo Real

Este lab enseÃ±a fundamentos para:

**DetecciÃ³n de Deriva de Modelos**:
- Rastrear rendimiento del modelo a lo largo del tiempo
- Alertar cuando la precisiÃ³n cae bajo el umbral
- Comparar nuevo modelo vs lÃ­nea base

**Pipelines de ML CI/CD**:
- Reentrenamiento automatizado con nuevos datos
- Versionado de modelos en pipeline de despliegue
- ReversiÃ³n a versiÃ³n anterior si hay problemas

**ColaboraciÃ³n en Equipo**:
- Historial de experimentos compartido
- Comparar modelos de compaÃ±eros de equipo
- Resultados reproducibles

**Despliegue en ProducciÃ³n**:
- Flujo de trabajo de promociÃ³n Staging â†’ ProducciÃ³n
- Pruebas A/B de diferentes versiones de modelo
- Despliegues blue-green

---

## ğŸ¢ Empresas que Usan MLflow

- **Netflix**: Experimentos de modelos de recomendaciÃ³n
- **Uber**: Versionado de modelos de precios y ETA
- **Databricks**: CreÃ³ MLflow, lo usa para todo ML
- **Microsoft**: IntegraciÃ³n con Azure ML
- **AWS**: Seguimiento de MLflow en SageMaker

---

## âœ… Criterios de Ã‰xito

Al final, podrÃ¡s:

1. Rastrear experimentos de ML con parÃ¡metros y mÃ©tricas
2. Comparar configuraciones de hiperparÃ¡metros
3. Seleccionar el mejor modelo de mÃºltiples opciones
4. Registrar modelos con control de versiones
5. Navegar la UI de MLflow con confianza
6. Entender flujos de trabajo de ML en producciÃ³n

**Tiempo Total**: ~50 minutos
**TODOs Totales**: 15 (todos claramente marcados)

Â¡Construyamos habilidades de ML listas para producciÃ³n! ğŸš€
