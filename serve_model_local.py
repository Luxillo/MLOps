#!/usr/bin/env python3
"""
Servidor Local del Modelo - Desde directorio ra√≠z

Sirve el modelo directamente usando Flask
"""

import mlflow.pyfunc
from flask import Flask, request, jsonify
import pandas as pd
import json

def main():
    # Cargar modelo
    print("üîÑ Cargando modelo...")
    try:
        model = mlflow.pyfunc.load_model("models:/DevOps-Response-Predictor@staging")
        print("‚úÖ Modelo cargado")
    except Exception as e:
        print(f"‚ùå Error cargando modelo: {e}")
        print("üí° Aseg√∫rate de completar la Tarea 4 primero")
        return

    # Crear app Flask
    app = Flask(__name__)

    @app.route('/invocations', methods=['POST'])
    def predict():
        """Endpoint de predicci√≥n"""
        try:
            # Obtener datos JSON
            data = request.get_json()
            
            # Convertir a DataFrame
            if 'dataframe_split' in data:
                df = pd.DataFrame(
                    data['dataframe_split']['data'],
                    columns=data['dataframe_split']['columns']
                )
            else:
                return jsonify({"error": "Formato de datos inv√°lido"}), 400
            
            # Hacer predicci√≥n
            predictions = model.predict(df)
            
            # Retornar resultado
            return jsonify({"predictions": predictions.tolist()})
            
        except Exception as e:
            return jsonify({"error": str(e)}), 500

    @app.route('/health', methods=['GET'])
    def health():
        """Health check"""
        return jsonify({"status": "ok"})

    print("üöÄ Iniciando servidor del modelo...")
    print("üì° API disponible en: http://localhost:8081/invocations")
    print("‚èπÔ∏è  Presiona Ctrl+C para detener")
    
    app.run(host='0.0.0.0', port=8081, debug=False)

if __name__ == "__main__":
    main()